# TreeRAG 项目核心算法与后续计划

## 1. 当前系统核心思想

我想要解决的问题：  
- GraphRAG offline构建KG时成本过高，效率很低，例如LightRAG构建一本西游记的KG需要近8个小时，且泛用性差，面对不同的case往往要设计多种prompt来进行entity/relation的抽取，且在面对大部分简单的问题，并不需要KG的辅助，naive RAG就可以取得不错的效果 
- GraphRAG 中 对entity进行merge非常困难，不同章节抽取的同一entity天然离的很远，merge策略的颗粒度难以调整，太粗导致图谱节点过多，噪音很大，太细导致出现 hub 节点，也就是某一个节点度数极高，无论使用agentic还是graph diffusion的方法，对后续检索非常不利
- 传统RAG对于总结，多跳问题不够有效，且RAG分chunk大小难以抉择，chunk size太大引入杂音，召回不够精准，太小检索到的内容过于零碎，llm难以推断有效的结论
- 无论是GraphRAG和普通的RAG对于 章节边界 考虑的都不够，而这恰恰是我们人类阅读文章时的习惯：对一个复杂问题先拆分->定位到不同的章节的具体内容->收集足够的信息进行回答

TreeRAG 的核心是“先树化，再分层检索，再证据生成”：

1. 先把文档组织成树结构（Section Tree），将文档的结构信息显式化。
2. 查询时先在树上定位最相关的叶子节点（而不是全量文本盲检索）。
3. 在候选叶子节点内部做混合检索（Dense + BM25），再跨节点重排。
4. 基于最终证据片段进行答案合成，并保留来源路径（heading path）。

这套设计的目标是降低检索噪声、提高可解释性，并兼顾语义召回与关键词精确匹配。

## 2. 核心算法流程（按实现）

### 2.1 文档树构建（Tree Builder）

- 仅识别 Markdown ATX 标题（`#` 到 `######`）。
- 采用“编号优先、井号兜底”的层级推断：
  - 如 `1.2.3` / `A.1.2` 按编号深度推断层级；
  - 无编号时按 `#` 数量推断层级；
  - 层级上限固定为 3。
- 使用栈构建父子关系，得到 `heading_path`。
- 对代码块（fence）内的伪标题进行忽略。
- 摘要采用自底向上递归生成：叶子由内容摘要，父节点由子摘要聚合。

### 2.2 索引构建（Indexing）

- 仅对叶子节点内容分块（默认 `chunk_size=200`, `overlap=50`， 此时因为已经定位到了相关内容在leaf上，使用细粒度的分词策略有助于提高召回精度）。
- 每个 chunk 生成 embedding（mock 哈希向量或在线 embedding）。
- 每个叶子节点内部独立构建 BM25 索引。
- 索引产物：`metadata.json`、`chunks.jsonl`、`embeddings.npy`、`bm25.pkl`。

### 2.3 查询推理（固定 3 步）

1. `Node Locating`：
   - LLM 根据树结构与节点摘要选择叶子节点并生成 `sub_query`；
   - 若失败自动回退到关键词匹配（heading + summary）。
2. `Hybrid Retrieval`：
   - 在每个候选叶子内部计算 dense 相似度与 BM25；
   - min-max 归一化后加权融合：
   - `fused = dense_weight * dense_norm + bm25_weight * bm25_norm`
3. `Rerank + Synthesis`：
   - 对候选 chunk 跨节点重排（失败回退 fused 顺序）；
   - 使用最终证据生成答案，要求基于证据、带来源路径。

## 3. 当前实现边界

1. tree 构建依赖正则与标题模式，当前更适合“结构清晰、标题明确”的文档。
2. 层级固定为 3 层，复杂文档结构表达能力有限。
3. 长 leaf 节点虽然被 chunk 化，但仍可能存在跨段依赖与推理链断裂问题。

## 4. 下一步重点

### 4.1 针对超长 leaf 节点做 KG 构建，增强推理

- 目标：把长 leaf 从“纯文本块检索”升级为“文本块 + 关系图”双通道检索。
- 建议策略：
  1. 对超长 leaf（如长度超过阈值）做实体/事件/关系抽取；
  2. 构建 leaf 内局部 KG（节点=实体/概念，边=关系/因果/比较）；
  3. 查询时先做图检索找到关键子图，再回贴原文 chunk 作为证据；
  4. 最终把“图路径 + 原文证据”一起交给生成模块，提升多跳推理质量。

为什么要在超长叶子节点构建kg？ 

- 1.如若全文构建kg 会有很大的hub degree很高的节点 （如我在使用lightrag的图构建方式构建关于nVidia的wiki百科的KG时，3w的字的文章，构建出的nvidia的node的degree高达134）无论是使用agent推理或者graph diffusion类的算法（如PPR）噪音和成本都非常高 

- 2.超长叶子节点内往往含有detail 细节的内容 构建kg有利于串联他们 构建高质量的检索内容

### 4.2 tree 构建从“规则匹配”走向“更全面的清晰构建”

- 当前是基于 `re` 的标题识别，后续计划升级为混合策略：
  1. 规则解析（保留高精度）；
  2. 版面/语义信号补全（处理弱标题、非标准编号、OCR 噪声）；
  3. 必要时引入 LLM/分类器做层级纠错与结构补全；

## 5. 现状数据（基于仓库已有 index 产物）

- `index_lightrag_md`：`leaf_count=24`，`chunk_count=463`，最长 leaf 约 `5852` 字符。
- `index_wpiea2025067_md`：`leaf_count=25`，`chunk_count=717`，最长 leaf 约 `16469` 字符。

结论：长 leaf 在当前数据中占比不低，优先推进“长 leaf 的 KG 化”是合理且必要的。


核心思想：充分利用文章已有的结构化 目录的信息 减少不必要的开支 面对多跳，总结类型问题，有双层导航
- llm agent负责‘explore’而不是‘retrieve’相关的章节信息，在大方向上决定搜索的scope，天生适合多跳问题
- leaf节点负责‘retrieve’更细节的内容，然后全局rerank