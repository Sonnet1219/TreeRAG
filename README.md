# Tree Builder

Tree Builder parses a Markdown document into a 3-level section tree, generates section summaries, prints an ASCII view in terminal, and exports JSON.

## Requirements

- Python 3.10+
- Optional: `python-dotenv` for `.env` loading (a built-in fallback loader is also provided)

## Install Dependencies

```bash
pip install -r requirements.txt
```

## Quick Start

```bash
python -m tree_builder.main tree_builder/test_data/test_standard.md --mode mock
```

This command:

1. Parses markdown headings into sections.
2. Infers heading levels using numbering-first fallback-to-hash logic.
3. Builds a tree with max depth 3.
4. Generates summaries with `MockSummarizer`.
5. Prints the tree and writes JSON to `tree_builder/test_data/test_standard.tree.json`.

## CLI

```bash
python -m tree_builder.main <input_markdown> [--mode mock|llm] [--provider openai|anthropic] [--output path.json]
```

- `--mode mock` (default): local truncation summary.
- `--mode llm`: real OpenAI-compatible chat completion for `--provider openai`.
- `--output`: optional output path. If omitted, output is `<input_stem>.tree.json` next to input file.

## LLM Configuration (.env)

Create or update `.env` in project root:

```bash
OPENAI_API_KEY=your_real_api_key
OPENAI_BASE_URL=https://api.openai.com/v1
OPENAI_MODEL=gpt-4o-mini
OPENAI_TIMEOUT_SECONDS=30
OPENAI_MAX_TOKENS=120
OPENAI_TEMPERATURE=0.2
```

Then run:

```bash
python -m tree_builder.main path/to/your.md --mode llm --provider openai
```

If `OPENAI_API_KEY` is missing, CLI exits with a clear error.
If you want to load a different env file, set `TREE_BUILDER_ENV_FILE=/path/to/custom.env`.

## Included Test Data

- `tree_builder/test_data/test_standard.md`
- `tree_builder/test_data/test_flat.md`

## Run Tests

```bash
python -m unittest discover -s tests -v
```

## Notes

- Only ATX headings are recognized (`#` to `######`).
- Numbered headings like `1.2.3` and `A.1.2` take priority for level inference.
- Heading levels are capped at 3.
- Headings inside fenced code blocks are ignored.

---

# TreeRAG Phase 2

TreeRAG Phase 2 adds a fixed 3-step retrieval pipeline:

1. `Node Locating`
2. `In-Node Hybrid Retrieval (Dense + BM25 + Rerank)`
3. `Synthesis`

It supports both:

- `.tree.json` input generated by `tree_builder`
- raw `.md` input (auto-parsed into tree during indexing)

## Phase 2 CLI

```bash
python -m tree_rag.main index --input <path.tree.json|path.md> --output <index_dir> [--mock]
python -m tree_rag.main query --index <index_dir> --query "your question" [--mock]
python -m tree_rag.main interactive --index <index_dir> [--mock]
```

## Phase 2 Configuration (.env)

TreeRAG uses OpenAI-compatible Qwen endpoints by default.

```bash
OPENAI_API_KEY=your_api_key
OPENAI_BASE_URL=https://dashscope.aliyuncs.com/compatible-mode/v1

RAG_LLM_MODEL=qwen-plus
RAG_EMBED_MODEL=text-embedding-v3
RAG_RERANK_MODEL=gte-rerank-v2

RAG_TIMEOUT_SECONDS=30
RAG_TOP_K=5
RAG_DENSE_WEIGHT=0.5
RAG_BM25_WEIGHT=0.5
```

`--mock` mode works fully offline and does not require API keys.

### Rerank Model (Qwen/OpenAI-compatible API key mode)

TreeRAG calls rerank through OpenAI-compatible `/rerank` endpoint.

Set:

- `OPENAI_API_KEY`: your API key
- `OPENAI_BASE_URL`: your provider compatible base URL
- `RAG_RERANK_MODEL`: your rerank model name (for example `gte-rerank-v2` or your Qwen rerank deployment name)

If rerank fails at runtime, the pipeline automatically falls back to fused dense+BM25 ranking.

## Index Artifacts

`index` command writes:

- `metadata.json`
- `chunks.jsonl`
- `embeddings.npy`
- `bm25.pkl`

## Example (Current Repo Data)

```bash
python -m tree_rag.main index \
  --input MinerU_markdown_2410.05779v3_2022302225920233472.tree.json \
  --output index_output \
  --mock

python -m tree_rag.main query \
  --index index_output \
  --query "LightRAG 的 dual-level retrieval 是什么？" \
  --mock
```

## Full Pipeline from Your Markdown File

```bash
python -m tree_rag.main index \
  --input MinerU_markdown_2410.05779v3_2022302225920233472.md \
  --output index_lightrag_md

python -m tree_rag.main query \
  --index index_lightrag_md \
  --query "LightRAG 的 dual-level retrieval 是什么？"

python -m tree_rag.main interactive \
  --index index_lightrag_md
```
