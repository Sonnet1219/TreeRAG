# TreeRAG â€” Robust Tree Builder å®Œæ•´é‡æ„æ–¹æ¡ˆ

## é—®é¢˜æ¦‚è¿°

å½“å‰ Tree Builder å®Œå…¨ä¾èµ–æ­£åˆ™åŒ¹é… `#` æ•°é‡å’Œç¼–å·æ¨¡å¼æ¨æ–­å±‚çº§ï¼Œæ— æ³•åº”å¯¹çœŸå®ä¸–ç•Œä¸­å„ç§éæ ‡å‡†çš„ Markdown æ–‡æ¡£ã€‚æœ¬æ–¹æ¡ˆè®¾è®¡ä¸€ä¸ªä¸‰å±‚é€’è¿›çš„å±‚çº§æ¨æ–­ç³»ç»Ÿï¼šè§„åˆ™å¼•æ“ï¼ˆé«˜ç½®ä¿¡åº¦å¿«é€Ÿå¤„ç†ï¼‰â†’ å¯å‘å¼æ¨æ–­ï¼ˆå¤„ç†æ¨¡ç³Šåœºæ™¯ï¼‰â†’ LLM å…œåº•ï¼ˆå¤„ç†æç«¯æƒ…å†µï¼‰ã€‚

---

## Part 1: å…¨éƒ¨è¾¹ç•Œåœºæ™¯æšä¸¾

### Case 1: æ ‡å‡† Markdownï¼ˆåŸºçº¿ï¼Œå·²æ”¯æŒï¼‰
```markdown
# 1 Introduction
## 1.1 Background
### 1.1.1 History
```
`#` æ•°é‡å’Œç¼–å·æ·±åº¦ä¸€è‡´ï¼Œæ— æ­§ä¹‰ã€‚

### Case 2: æ‰å¹³æ ‡é¢˜ + æ•°å­—ç¼–å·ï¼ˆå·²éƒ¨åˆ†æ”¯æŒï¼‰
```markdown
# 1 Introduction
# 1.1 Background
# 1.1.1 History
```
æ‰€æœ‰æ ‡é¢˜éƒ½ç”¨ `#`ï¼Œä½†ç¼–å·æš—ç¤ºå±‚çº§ã€‚

### Case 3: æ‰å¹³æ ‡é¢˜ + æ— ç¼–å·
```markdown
# Introduction
# Background
# Motivation
# Methods
# Data Collection
# Model Architecture
# Experiments
# Conclusion
```
æ²¡æœ‰ä»»ä½•ç¼–å·ï¼Œæ‰€æœ‰æ ‡é¢˜éƒ½æ˜¯ `#`ï¼Œæ— æ³•åŒºåˆ†å“ªäº›æ˜¯çˆ¶ã€å“ªäº›æ˜¯å­ã€‚è¿™æ˜¯æœ€éš¾çš„æƒ…å†µã€‚

### Case 4: ç¼–å·è·³è·ƒ / ä¸è¿ç»­
```markdown
# 1 Introduction
# 3 Methods          â† è·³è¿‡äº† 2
# 3.1 Overview
# 3.3 Training       â† è·³è¿‡äº† 3.2
```
ç¼–å·ä¸è¿ç»­ä½†å±‚çº§å…³ç³»ä»å¯æ¨æ–­ã€‚

### Case 5: `#` å±‚çº§è·³è·ƒ
```markdown
# Introduction
### Detail A          â† ç›´æ¥ä» # è·³åˆ° ###ï¼Œç¼ºå°‘ ##
### Detail B
## Methods
```
ä¸­é—´å±‚çº§ç¼ºå¤±ã€‚

### Case 6: æ··åˆç¼–å·æ ¼å¼
```markdown
# 1. Introduction
# 1.1 Background
# II. Related Work          â† ç½—é©¬æ•°å­—
# A. Appendix               â† å­—æ¯ç¼–å·
# A.1 Dataset Details
```
åŒä¸€æ–‡æ¡£å†…å¤šç§ç¼–å·é£æ ¼æ··ç”¨ã€‚

### Case 7: ç‰¹æ®Šå›ºå®šç« èŠ‚ï¼ˆå­¦æœ¯è®ºæ–‡å¸¸è§ï¼‰
```markdown
# Abstract
# 1 Introduction
# 2 Methods
...
# 5 Conclusion
# Acknowledgments
# References
# Appendix A: Supplementary Results
# Appendix B: Proofs
```
Abstractã€Referencesã€Acknowledgmentsã€Appendix ç­‰æ— ç¼–å·ä½†å±äºä¸€çº§ç« èŠ‚ã€‚

### Case 8: ä»£ç å—å†…çš„ä¼ªæ ‡é¢˜
````markdown
# Real Heading

```python
# This is a comment, not a heading
## Another comment
```

## Another Real Heading
````
ä»£ç å— fence å†…çš„ `#` ä¸åº”è¢«è¯†åˆ«ä¸ºæ ‡é¢˜ã€‚

### Case 9: Markdown æ ¼å¼å™ªéŸ³
```markdown
#Introduction          â† # åæ— ç©ºæ ¼ï¼ˆéæ ‡å‡†ä½†å¸¸è§ï¼‰
##  Background         â† å¤šä½™ç©ºæ ¼
# **1.1 Motivation**   â† æ ‡é¢˜å†…å« bold æ ‡è®°
# [2 Methods](#methods) â† æ ‡é¢˜å†…å«é“¾æ¥
# 3. Methods.          â† æœ«å°¾å¤šä½™å¥å·
```

### Case 10: éæ ‡å‡†ç¼–å·æ¨¡å¼
```markdown
# Section One
# Section Two
# ç¬¬ä¸€ç«  ç»ªè®º           â† ä¸­æ–‡ç¼–å·
# ç¬¬äºŒç«  æ–¹æ³•
# Chapter 3: Results
# Part IV: Discussion   â† ç½—é©¬æ•°å­— + Part
```

### Case 11: å±‚çº§åè½¬ / ä¸ä¸€è‡´
```markdown
## Overview             â† æ–‡æ¡£ä»¥ ## å¼€å¤´
## Background
### Details
## Methods
# Conclusion            â† çªç„¶å‡ºç° #ï¼Œæ¯”å‰é¢çš„ ## æ›´é«˜
```
`#` å±‚çº§ä½¿ç”¨ä¸ä¸€è‡´ã€‚

### Case 12: é‡å¤æ ‡é¢˜æ–‡æœ¬
```markdown
# Overview
## Methods
### Overview            â† å’Œé¡¶å±‚æ ‡é¢˜åŒå
## Results
### Summary
# Summary               â† å’Œå­æ ‡é¢˜åŒå
```

### Case 13: è¶…æ·±å±‚çº§
```markdown
# 1 Introduction
## 1.1 Background
### 1.1.1 History
#### 1.1.1.1 Early Work       â† è¶…è¿‡ 3 å±‚
##### 1.1.1.1.1 Foundations   â† ç¬¬ 5 å±‚
```
è¶…è¿‡ max_depth é™åˆ¶çš„æ·±å±‚åµŒå¥—ã€‚

### Case 14: çº¯ PDF è½¬æ¢äº§ç‰©ï¼ˆæ— æ ‡é¢˜æ ‡è®°ï¼‰
```markdown
Introduction

This paper presents...

1.1 Background

The field of...
```
æ²¡æœ‰ `#` æ ‡è®°ï¼Œæ ‡é¢˜é ç‹¬ç«‹çŸ­è¡Œ + ç¼–å·æ¨æ–­ã€‚

---

## Part 2: ä¸‰å±‚é€’è¿›æ¶æ„

```
è¾“å…¥: Markdown åŸæ–‡
         â”‚
         â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  Layer 1: é¢„å¤„ç†  â”‚  æ¸…æ´—å™ªéŸ³ã€è¯†åˆ«ä»£ç å—ã€æ ‡å‡†åŒ–æ ¼å¼
   â”‚  (ç¡®å®šæ€§è§„åˆ™)     â”‚  
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚ å¹²å‡€çš„ heading åˆ—è¡¨
            â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  Layer 2: è§„åˆ™   â”‚  ç¼–å·è§£æã€# æ•°é‡ã€ç‰¹æ®Šç« èŠ‚è¯†åˆ«
   â”‚  + å¯å‘å¼æ¨æ–­    â”‚  æ¯ä¸ª heading äº§å‡º (inferred_level, confidence)
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚ å¸¦ç½®ä¿¡åº¦çš„å±‚çº§åˆ—è¡¨
            â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  Layer 3: LLM   â”‚  ä»…å¯¹ confidence < é˜ˆå€¼çš„ heading è°ƒç”¨ LLM
   â”‚  è¾…åŠ©ä¿®æ­£        â”‚  ä¿®æ­£å±‚çº§ã€è¡¥å…¨ç»“æ„
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚ æœ€ç»ˆç¡®å®šçš„å±‚çº§åˆ—è¡¨
            â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  æ ‘æ„å»º + éªŒè¯    â”‚  æ ˆç®—æ³•æ„å»ºæ ‘ + ç»“æ„å®Œæ•´æ€§æ ¡éªŒ
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Part 3: Layer 1 â€” é¢„å¤„ç†

### 3.1 ä»£ç å—è¿‡æ»¤

è¯†åˆ« fenced code blocksï¼ˆ``` æˆ– ~~~ï¼‰ï¼Œæ ‡è®°å…¶è¡ŒèŒƒå›´ï¼Œåç»­è§£æè·³è¿‡è¿™äº›è¡Œã€‚

```python
def mark_code_blocks(lines: List[str]) -> Set[int]:
    """è¿”å›å±äºä»£ç å—å†…éƒ¨çš„è¡Œå·é›†åˆ"""
    code_lines = set()
    in_code = False
    fence_pattern = re.compile(r'^(`{3,}|~{3,})')

    for i, line in enumerate(lines):
        if fence_pattern.match(line.strip()):
            if in_code:
                code_lines.add(i)  # é—­åˆè¡Œä¹Ÿæ ‡è®°
                in_code = False
            else:
                code_lines.add(i)
                in_code = True
        elif in_code:
            code_lines.add(i)

    return code_lines
```

### 3.2 æ ‡é¢˜è¡Œæ ‡å‡†åŒ–

æ¸…æ´—å„ç§æ ¼å¼å™ªéŸ³ï¼Œæå–å¹²å‡€çš„æ ‡é¢˜ä¿¡æ¯ï¼š

```python
def normalize_heading(raw_line: str) -> Optional[dict]:
    """
    è¾“å…¥: åŸå§‹è¡Œæ–‡æœ¬
    è¾“å‡º: {
        "hash_count": int,           # åŸå§‹ # æ•°é‡
        "raw_text": str,             # æ¸…æ´—åçš„æ ‡é¢˜æ–‡æœ¬
        "has_hash_marker": bool,     # æ˜¯å¦æœ‰ # æ ‡è®°
    }
    """
    # åŒ¹é…æ ‡å‡† ATX æ ‡é¢˜: # åå¿…é¡»æœ‰ç©ºæ ¼ï¼ˆæˆ– # åæ— ç©ºæ ¼ä½†ç´§è·Ÿå­—æ¯â€”â€”å®½æ¾æ¨¡å¼ï¼‰
    # æ ‡å‡†: r'^(#{1,6})\s+(.+)'
    # å®½æ¾: r'^(#{1,6})(.+)'  å½“æ ‡å‡†åŒ¹é…å¤±è´¥æ—¶å°è¯•
    
    # æ¸…æ´—æ“ä½œ:
    # 1. å»é™¤ bold/italic æ ‡è®°: **text** â†’ text, *text* â†’ text
    # 2. å»é™¤é“¾æ¥: [text](url) â†’ text
    # 3. å»é™¤æœ«å°¾å¤šä½™æ ‡ç‚¹: "3. Methods." â†’ "3. Methods"
    # 4. å»é™¤é¦–å°¾ç©ºç™½
    # 5. å»é™¤æœ«å°¾çš„ # (ATX closing): "## Title ##" â†’ "Title"
```

### 3.3 æ—  `#` æ ‡è®°çš„æ ‡é¢˜æ£€æµ‹ï¼ˆCase 14ï¼‰

å¯¹äºæ²¡æœ‰ `#` çš„è¡Œï¼Œç”¨å¯å‘å¼åˆ¤æ–­æ˜¯å¦ä¸ºæ ‡é¢˜ï¼š

```python
def detect_unmarked_heading(line: str, prev_line: str, next_line: str) -> bool:
    """
    æ—  # æ ‡è®°çš„æ½œåœ¨æ ‡é¢˜æ£€æµ‹
    ç‰¹å¾:
    - ç‹¬ç«‹çŸ­è¡Œï¼ˆ< 80 å­—ç¬¦ï¼‰
    - å‰åæœ‰ç©ºè¡Œ
    - ä»¥æ•°å­—ç¼–å·å¼€å¤´ï¼ˆå¦‚ "1.1 Background"ï¼‰
    - æˆ–åŒ¹é…å·²çŸ¥ç« èŠ‚åï¼ˆå¦‚ "Introduction", "Methods"ï¼‰
    - ä¸å«å¥å·ç»“å°¾ï¼ˆæ ‡é¢˜é€šå¸¸ä¸ä»¥å¥å·ç»“å°¾ï¼‰
    - ä¸å«é€—å·ï¼ˆæ ‡é¢˜é€šå¸¸ä¸å«é€—å·ï¼‰
    """
```

---

## Part 4: Layer 2 â€” è§„åˆ™ + å¯å‘å¼æ¨æ–­

### 4.1 ä¿¡å·æå–

å¯¹æ¯ä¸ªè¯†åˆ«åˆ°çš„ headingï¼Œæå–å¤šç§å±‚çº§ä¿¡å·ï¼š

```python
@dataclass
class HeadingSignals:
    """ä»å•ä¸ª heading ä¸­æå–çš„æ‰€æœ‰å±‚çº§ä¿¡å·"""

    # --- ä¿¡å· 1: # æ•°é‡ ---
    hash_count: int                    # åŸå§‹ # æ•°é‡ (0 if no # marker)
    has_hash_marker: bool              # æ˜¯å¦æœ‰ # æ ‡è®°

    # --- ä¿¡å· 2: ç¼–å·æ¨¡å¼ ---
    numbering: Optional[str]           # åŸå§‹ç¼–å·, e.g., "1.2.3", "A.1", "IV"
    numbering_type: Optional[str]      # "arabic" | "roman" | "letter" | "chinese" | None
    numbering_depth: int               # ç¼–å·å±‚çº§æ·±åº¦ (0 if no numbering)
    # "1" â†’ 1, "1.2" â†’ 2, "1.2.3" â†’ 3, "A" â†’ 1, "A.1" â†’ 2

    # --- ä¿¡å· 3: ç‰¹æ®Šç« èŠ‚ ---
    is_special_section: bool           # Abstract, References, Acknowledgments, Appendix ç­‰
    special_section_level: int         # ç‰¹æ®Šç« èŠ‚çš„é»˜è®¤å±‚çº§ (é€šå¸¸ä¸º 1)

    # --- ä¿¡å· 4: æ–‡æœ¬ç‰¹å¾ ---
    text_length: int                   # æ ‡é¢˜æ–‡æœ¬é•¿åº¦
    heading_text: str                  # æ¸…æ´—åçš„çº¯æ ‡é¢˜æ–‡æœ¬ï¼ˆå»é™¤ç¼–å·ï¼‰
```

### 4.2 ç¼–å·è§£æå™¨ï¼ˆæ‰©å±•ç‰ˆï¼‰

è¦†ç›–å„ç§ç¼–å·æ ¼å¼ï¼š

```python
NUMBERING_PATTERNS = [
    # é˜¿æ‹‰ä¼¯æ•°å­—: "1", "1.2", "1.2.3"
    (re.compile(r'^(\d+(?:\.\d+)*)[\.\s\)\:\-]?\s*(.+)'), "arabic"),

    # å­—æ¯ç¼–å·: "A", "A.1", "A.1.2"
    (re.compile(r'^([A-Z](?:\.\d+)*)[\.\s\)\:\-]?\s*(.+)'), "letter"),

    # ç½—é©¬æ•°å­—: "I", "II", "IV", "XI"
    (re.compile(r'^((?:X{0,3})(?:IX|IV|V?I{0,3}))[\.\s\)\:\-]\s*(.+)', re.IGNORECASE), "roman"),

    # ä¸­æ–‡ç¼–å·: "ç¬¬ä¸€ç« ", "ç¬¬äºŒèŠ‚"
    (re.compile(r'^ç¬¬([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾]+)[ç« èŠ‚éƒ¨åˆ†ç¯‡]\s*(.*)'), "chinese"),

    # Chapter/Part/Section å‰ç¼€: "Chapter 3", "Part II", "Section 4.1"
    (re.compile(r'^(?:Chapter|Part|Section)\s+(.+?)[\.\:\s]\s*(.+)', re.IGNORECASE), "prefix"),

    # Appendix: "Appendix A", "Appendix B.1"
    (re.compile(r'^Appendix\s+([A-Z](?:\.\d+)*)[\.\:\s]?\s*(.*)', re.IGNORECASE), "appendix"),
]
```

### 4.3 ç‰¹æ®Šç« èŠ‚è¯†åˆ«

```python
SPECIAL_SECTIONS = {
    # name pattern â†’ default level
    "abstract": 1,
    "æ‘˜è¦": 1,
    "introduction": 1,
    "å¼•è¨€": 1,
    "ç»ªè®º": 1,
    "related work": 1,
    "background": 1,       # æ³¨æ„: å¯èƒ½æ˜¯ 1 çº§ä¹Ÿå¯èƒ½æ˜¯ 2 çº§ï¼Œéœ€è¦ä¸Šä¸‹æ–‡åˆ¤æ–­
    "methodology": 1,
    "methods": 1,
    "method": 1,
    "approach": 1,
    "experiments": 1,
    "evaluation": 1,
    "results": 1,
    "discussion": 1,
    "conclusion": 1,
    "conclusions": 1,
    "summary": 1,
    "acknowledgments": 1,
    "acknowledgements": 1,
    "references": 1,
    "bibliography": 1,
    "appendix": 1,
    "supplementary": 1,
    "future work": 1,      # å¯èƒ½ 1 çº§ä¹Ÿå¯èƒ½ 2 çº§
}

def match_special_section(heading_text: str) -> Optional[int]:
    """æ¨¡ç³ŠåŒ¹é…ç‰¹æ®Šç« èŠ‚åï¼Œè¿”å›é»˜è®¤å±‚çº§æˆ– None"""
    normalized = heading_text.lower().strip()
    for pattern, level in SPECIAL_SECTIONS.items():
        if normalized == pattern or normalized.startswith(pattern):
            return level
    return None
```

### 4.4 å±‚çº§æ¨æ–­è§„åˆ™å¼•æ“

ç»¼åˆæ‰€æœ‰ä¿¡å·ï¼ŒæŒ‰ä¼˜å…ˆçº§æ¨æ–­å±‚çº§å¹¶ç»™å‡ºç½®ä¿¡åº¦ï¼š

```python
def infer_level(signals: HeadingSignals, context: 'DocumentContext') -> Tuple[int, float]:
    """
    è¿”å› (inferred_level, confidence)
    confidence: 0.0 ~ 1.0ï¼Œä½äºé˜ˆå€¼(å¦‚ 0.6)æ—¶è§¦å‘ LLM ä¿®æ­£
    """

    # =============================================
    # Rule 1: ç¼–å·æ·±åº¦ï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼Œæœ€å¯é ï¼‰
    # =============================================
    if signals.numbering_depth > 0:
        level = min(signals.numbering_depth, MAX_DEPTH)

        if signals.has_hash_marker and signals.hash_count == level:
            # ç¼–å·å’Œ # æ•°é‡ä¸€è‡´ â†’ æœ€é«˜ç½®ä¿¡
            return level, 1.0
        elif signals.has_hash_marker and signals.hash_count != level:
            # ç¼–å·å’Œ # æ•°é‡ä¸ä¸€è‡´ â†’ ä¿¡ä»»ç¼–å·ï¼ˆå¸¸è§äºæ‰å¹³ markdownï¼‰
            return level, 0.9
        else:
            # æœ‰ç¼–å·æ—  # â†’ ä¿¡ä»»ç¼–å·
            return level, 0.85

    # =============================================
    # Rule 2: ç‰¹æ®Šç« èŠ‚åï¼ˆæ— ç¼–å·æ—¶çš„å¼ºä¿¡å·ï¼‰
    # =============================================
    if signals.is_special_section:
        level = signals.special_section_level
        # å¦‚æœ # æ•°é‡ä¸ç‰¹æ®Šç« èŠ‚é»˜è®¤å±‚çº§ä¸€è‡´ï¼Œç½®ä¿¡åº¦æ›´é«˜
        if signals.has_hash_marker and signals.hash_count == level:
            return level, 0.9
        elif signals.has_hash_marker:
            return level, 0.7  # # æ•°é‡å’Œé¢„æœŸä¸ä¸€è‡´ï¼Œé™ä½ç½®ä¿¡
        else:
            return level, 0.75

    # =============================================
    # Rule 3: çº¯ # æ•°é‡ï¼ˆæ— ç¼–å·ã€éç‰¹æ®Šç« èŠ‚ï¼‰
    # =============================================
    if signals.has_hash_marker:
        level = min(signals.hash_count, MAX_DEPTH)

        # æ£€æŸ¥æ˜¯å¦å’Œä¸Šä¸‹æ–‡ä¸­çš„å…¶ä»– heading ä¸€è‡´
        consistency = context.check_hash_consistency()
        if consistency == "consistent":
            # æ–‡æ¡£ä¸­ # æ•°é‡ä½¿ç”¨ä¸€è‡´ â†’ è¾ƒé«˜ç½®ä¿¡
            return level, 0.8
        elif consistency == "all_same":
            # æ‰€æœ‰æ ‡é¢˜éƒ½ç”¨åŒä¸€ä¸ª # æ•°é‡ï¼ˆå¦‚å…¨æ˜¯ #ï¼‰â†’ ä½ç½®ä¿¡ï¼Œæ— æ³•åŒºåˆ†å±‚çº§
            return level, 0.3  # éœ€è¦ LLM ä»‹å…¥
        else:
            return level, 0.5  # éƒ¨åˆ†ä¸ä¸€è‡´ï¼Œä¸­ç­‰ç½®ä¿¡

    # =============================================
    # Rule 4: æ—  # æ ‡è®°ã€æ— ç¼–å·ï¼ˆæœ€ä½ç½®ä¿¡ï¼‰
    # =============================================
    return 1, 0.2  # å‡ ä¹ä¸€å®šéœ€è¦ LLM ä»‹å…¥
```

### 4.5 ä¸Šä¸‹æ–‡æ„ŸçŸ¥æ¨æ–­ï¼ˆDocumentContextï¼‰

æŸäº›å±‚çº§åˆ¤æ–­éœ€è¦å…¨å±€ä¸Šä¸‹æ–‡ï¼š

```python
class DocumentContext:
    """æ”¶é›†æ–‡æ¡£çº§åˆ«çš„å…¨å±€ä¿¡å·ï¼Œè¾…åŠ©å•ä¸ª heading çš„å±‚çº§æ¨æ–­"""

    def __init__(self, all_headings: List[HeadingSignals]):
        self.all_headings = all_headings

        # ç»Ÿè®¡ # ä½¿ç”¨æ¨¡å¼
        self.hash_distribution = Counter(h.hash_count for h in all_headings if h.has_hash_marker)

        # ç»Ÿè®¡ç¼–å·ä½¿ç”¨æ¨¡å¼
        self.has_any_numbering = any(h.numbering_depth > 0 for h in all_headings)
        self.numbering_coverage = sum(1 for h in all_headings if h.numbering_depth > 0) / len(all_headings)

    def check_hash_consistency(self) -> str:
        """
        æ£€æŸ¥ # æ•°é‡çš„ä½¿ç”¨æ˜¯å¦ä¸€è‡´
        è¿”å›:
        - "consistent": å¤šç§ # å±‚çº§ä¸”ä½¿ç”¨åˆç†
        - "all_same": æ‰€æœ‰æ ‡é¢˜ç”¨åŒä¸€ # æ•°é‡ï¼ˆå¦‚å…¨æ˜¯ #ï¼‰
        - "inconsistent": ä½¿ç”¨æ··ä¹±
        """
        if len(self.hash_distribution) == 1:
            return "all_same"
        elif len(self.hash_distribution) >= 2:
            return "consistent"
        else:
            return "inconsistent"

    def get_dominant_numbering_type(self) -> Optional[str]:
        """è·å–æ–‡æ¡£ä¸­æœ€ä¸»è¦çš„ç¼–å·ç±»å‹"""
        types = [h.numbering_type for h in self.all_headings if h.numbering_type]
        if not types:
            return None
        return Counter(types).most_common(1)[0][0]
```

---

## Part 5: Layer 3 â€” LLM è¾…åŠ©ä¿®æ­£

### 5.1 ä½•æ—¶è§¦å‘ LLM

```python
LLM_CONFIDENCE_THRESHOLD = 0.6

def needs_llm_correction(headings_with_levels: List[Tuple[HeadingSignals, int, float]]) -> bool:
    """åˆ¤æ–­æ˜¯å¦éœ€è¦ LLM ä»‹å…¥"""
    low_confidence_count = sum(1 for _, _, conf in headings_with_levels if conf < LLM_CONFIDENCE_THRESHOLD)
    low_confidence_ratio = low_confidence_count / len(headings_with_levels)

    # æ¡ä»¶ 1: è¶…è¿‡ 30% çš„ heading ä½ç½®ä¿¡
    if low_confidence_ratio > 0.3:
        return True

    # æ¡ä»¶ 2: å…¨éƒ¨ heading ç”¨åŒä¸€ # æ•°é‡ä¸”æ— ç¼–å·
    if all(h.hash_count == headings_with_levels[0][0].hash_count for h, _, _ in headings_with_levels):
        if not any(h.numbering_depth > 0 for h, _, _ in headings_with_levels):
            return True

    # æ¡ä»¶ 3: å­˜åœ¨å±‚çº§è·³è·ƒï¼ˆå¦‚ 1 â†’ 3ï¼Œè·³è¿‡äº† 2ï¼‰
    levels = [lv for _, lv, _ in headings_with_levels]
    for i in range(1, len(levels)):
        if levels[i] - levels[i-1] > 1:  # å‘ä¸‹è·³è·ƒè¶…è¿‡ 1 çº§
            return True

    return False
```

### 5.2 LLM ä¿®æ­£æ¨¡å¼

æœ‰ä¸¤ç§è°ƒç”¨æ¨¡å¼ï¼Œæ ¹æ®æƒ…å†µé€‰æ‹©ï¼š

#### æ¨¡å¼ A: å…¨é‡ç»“æ„æ¨æ–­ï¼ˆä½ç½®ä¿¡æ¯”ä¾‹é«˜æ—¶ä½¿ç”¨ï¼‰

æŠŠæ‰€æœ‰ heading ä¸€æ¬¡æ€§å‘ç»™ LLMï¼Œè®©å®ƒæ¨æ–­å®Œæ•´çš„å±‚çº§ç»“æ„ï¼š

```
ä½ æ˜¯ä¸€ä¸ªæ–‡æ¡£ç»“æ„åˆ†æä¸“å®¶ã€‚ä»¥ä¸‹æ˜¯ä»ä¸€ç¯‡æ–‡æ¡£ä¸­æå–çš„æ‰€æœ‰ç« èŠ‚æ ‡é¢˜ï¼ˆæŒ‰å‡ºç°é¡ºåºï¼‰ã€‚
è¯·ä¸ºæ¯ä¸ªæ ‡é¢˜æ¨æ–­å…¶åœ¨æ–‡æ¡£ä¸­çš„å±‚çº§ï¼ˆ1=ä¸€çº§æ ‡é¢˜, 2=äºŒçº§æ ‡é¢˜, 3=ä¸‰çº§æ ‡é¢˜ï¼‰ã€‚

æ¨æ–­ä¾æ®:
1. æ ‡é¢˜çš„ç¼–å·æ¨¡å¼ï¼ˆå¦‚ "1.2.3" æš—ç¤ºä¸‰çº§æ ‡é¢˜ï¼‰
2. æ ‡é¢˜çš„å†…å®¹è¯­ä¹‰ï¼ˆå¦‚ "Introduction" é€šå¸¸æ˜¯ä¸€çº§æ ‡é¢˜ï¼‰
3. æ ‡é¢˜ä¹‹é—´çš„é€»è¾‘å…³ç³»ï¼ˆå¦‚ "Background" é€šå¸¸ä»å±äº "Introduction"ï¼‰
4. æ–‡æ¡£çš„æ•´ä½“ç»“æ„æ¨¡å¼

æ ‡é¢˜åˆ—è¡¨:
{headings_list}

å¯¹æ¯ä¸ªæ ‡é¢˜ä½ è¿˜å¯ä»¥å‚è€ƒä»¥ä¸‹è§„åˆ™å¼•æ“çš„åˆæ­¥æ¨æ–­ç»“æœå’Œç½®ä¿¡åº¦ï¼š
{rule_based_results}

è¾“å‡ºä¸¥æ ¼ JSON:
[
  {{"index": 0, "heading": "...", "level": 1, "reasoning": "..."}},
  {{"index": 1, "heading": "...", "level": 2, "reasoning": "..."}},
  ...
]
```

#### æ¨¡å¼ B: å±€éƒ¨ä¿®æ­£ï¼ˆå°‘é‡ä½ç½®ä¿¡æ—¶ä½¿ç”¨ï¼‰

åªæŠŠä½ç½®ä¿¡çš„ heading åŠå…¶ä¸Šä¸‹æ–‡å‘ç»™ LLMï¼š

```
ä½ æ˜¯ä¸€ä¸ªæ–‡æ¡£ç»“æ„åˆ†æä¸“å®¶ã€‚ä»¥ä¸‹æ–‡æ¡£ç»“æ„ä¸­æœ‰å‡ ä¸ªæ ‡é¢˜çš„å±‚çº§ä¸ç¡®å®šï¼ˆæ ‡è®°ä¸º [?]ï¼‰ã€‚
è¯·æ ¹æ®ä¸Šä¸‹æ–‡æ¨æ–­å®ƒä»¬çš„æ­£ç¡®å±‚çº§ã€‚

æ–‡æ¡£ç»“æ„ï¼ˆå·²ç¡®å®šçš„éƒ¨åˆ†ï¼‰:
[L1] 1 Introduction
[L2] 1.1 Background
[?]  Background Details        â† éœ€è¦æ¨æ–­
[L2] 1.2 Motivation
[L1] 2 Methods
[?]  Data Preprocessing        â† éœ€è¦æ¨æ–­
[L2] 2.1 Model Architecture

å¯¹æ ‡è®°ä¸º [?] çš„æ ‡é¢˜ï¼Œè¾“å‡ºå…¶å±‚çº§:
[
  {{"heading": "Background Details", "level": 3, "reasoning": "ä»å±äº 1.1 Background"}},
  {{"heading": "Data Preprocessing", "level": 2, "reasoning": "ä¸ 2.1 å¹¶åˆ—ï¼ŒåŒå± 2 Methods"}}
]
```

#### é€‰æ‹©é€»è¾‘

```python
def select_llm_mode(headings_with_levels):
    low_conf_count = sum(1 for _, _, c in headings_with_levels if c < LLM_CONFIDENCE_THRESHOLD)
    total = len(headings_with_levels)

    if low_conf_count / total > 0.5:
        return "full"       # è¶…è¿‡ä¸€åŠä¸ç¡®å®š â†’ å…¨é‡æ¨æ–­
    else:
        return "partial"    # å°‘é‡ä¸ç¡®å®š â†’ å±€éƒ¨ä¿®æ­£
```

### 5.3 LLM ç»“æœåˆå¹¶

```python
def merge_llm_corrections(
    rule_results: List[Tuple[HeadingSignals, int, float]],
    llm_results: List[dict]
) -> List[Tuple[HeadingSignals, int, float]]:
    """
    å°† LLM çš„æ¨æ–­ç»“æœåˆå¹¶å›è§„åˆ™å¼•æ“çš„ç»“æœ
    
    ç­–ç•¥:
    - å¯¹äº confidence >= threshold çš„ heading: ä¿ç•™è§„åˆ™å¼•æ“ç»“æœ
    - å¯¹äº confidence < threshold çš„ heading: é‡‡ç”¨ LLM ç»“æœï¼Œç½®ä¿¡åº¦è®¾ä¸º 0.85
    - å¦‚æœ LLM ç»“æœå’Œè§„åˆ™å¼•æ“å†²çªä¸”è§„åˆ™å¼•æ“ç½®ä¿¡åº¦è¾ƒé«˜ï¼ˆ>= 0.8ï¼‰ï¼Œä¿ç•™è§„åˆ™å¼•æ“
    """
```

---

## Part 6: æ ‘æ„å»º + åå¤„ç†éªŒè¯

### 6.1 æ ˆç®—æ³•æ„å»ºæ ‘ï¼ˆä¸ä¹‹å‰ä¸€è‡´ï¼‰

å±‚çº§ç¡®å®šåï¼Œä½¿ç”¨æ ˆç®—æ³•æ„å»ºæ ‘ã€‚

### 6.2 ç»“æ„éªŒè¯ + è‡ªåŠ¨ä¿®å¤

```python
def validate_and_fix_tree(root: TreeNode) -> List[str]:
    """
    éªŒè¯æ ‘ç»“æ„çš„åˆç†æ€§ï¼Œè‡ªåŠ¨ä¿®å¤å¸¸è§é—®é¢˜
    è¿”å›ä¿®å¤æ—¥å¿—
    """
    fixes = []

    # Check 1: å±‚çº§è·³è·ƒä¿®å¤
    # å¦‚æœæŸä¸ªèŠ‚ç‚¹çš„ level æ¯” parent.level å¤§ 2 ä»¥ä¸Šï¼Œæ’å…¥è™šæ‹Ÿä¸­é—´èŠ‚ç‚¹
    # ä¾‹å¦‚: L1 â†’ L3ï¼ˆç¼ºå°‘ L2ï¼‰ï¼Œæ’å…¥ä¸€ä¸ª "[Inferred Section]" L2 èŠ‚ç‚¹
    for node in traverse_all(root):
        if node.parent and node.level > node.parent.level + 1:
            gap = node.level - node.parent.level - 1
            fixes.append(f"Level gap detected: {node.heading} (L{node.level}) under {node.parent.heading} (L{node.parent.level})")
            # ä¸æ’å…¥è™šæ‹ŸèŠ‚ç‚¹ï¼Œè€Œæ˜¯å°† node çš„ level ä¸‹è°ƒ
            node.level = node.parent.level + 1
            fixes.append(f"  â†’ Adjusted to L{node.level}")

    # Check 2: è¶…æ·±å±‚çº§æˆªæ–­
    for node in traverse_all(root):
        if node.level > MAX_DEPTH:
            old_level = node.level
            node.level = MAX_DEPTH
            fixes.append(f"Depth overflow: {node.heading} L{old_level} â†’ L{MAX_DEPTH}")

    # Check 3: å­¤å„¿èŠ‚ç‚¹æ£€æµ‹
    # å¦‚æœæŸä¸ªèŠ‚ç‚¹æ²¡æœ‰ parentï¼ˆé™¤ root å¤–ï¼‰ï¼ŒæŒ‚åˆ° root ä¸‹
    for node in traverse_all(root):
        if node.parent is None and node != root:
            node.parent = root
            root.children.append(node)
            node.level = 1
            fixes.append(f"Orphan node adopted: {node.heading}")

    # Check 4: ç©ºèŠ‚ç‚¹å‰ªæ
    # å¦‚æœæŸä¸ªéå¶å­èŠ‚ç‚¹æ—¢æ²¡æœ‰ content ä¹Ÿæ²¡æœ‰ childrenï¼Œåˆ é™¤
    for node in list(traverse_all(root)):
        if not node.is_leaf and not node.children and not node.content.strip():
            if node.parent:
                node.parent.children.remove(node)
                fixes.append(f"Empty node pruned: {node.heading}")

    return fixes
```

---

## Part 7: å®Œæ•´æ„å»ºä¸»æµç¨‹

```python
def build_robust_tree(markdown_text: str, doc_id: str, llm_client=None) -> Tuple[TreeNode, dict]:
    """
    è¿”å›: (root, build_report)
    build_report åŒ…å«æ„å»ºè¿‡ç¨‹çš„è¯¦ç»†æ—¥å¿—
    """
    lines = markdown_text.split('\n')
    report = {"warnings": [], "fixes": [], "llm_used": False}

    # ===== Layer 1: é¢„å¤„ç† =====
    code_block_lines = mark_code_blocks(lines)
    raw_headings = []
    for i, line in enumerate(lines):
        if i in code_block_lines:
            continue
        heading = normalize_heading(line)
        if heading:
            raw_headings.append(heading)
        elif detect_unmarked_heading(line, ...):
            raw_headings.append({"hash_count": 0, "raw_text": line.strip(), "has_hash_marker": False})

    # ===== Layer 2: ä¿¡å·æå– + è§„åˆ™æ¨æ–­ =====
    signals_list = [extract_signals(h) for h in raw_headings]
    context = DocumentContext(signals_list)

    rule_results = []  # List of (signals, level, confidence)
    for signals in signals_list:
        level, conf = infer_level(signals, context)
        rule_results.append((signals, level, conf))

    # ===== Layer 3: LLM ä¿®æ­£ï¼ˆæŒ‰éœ€ï¼‰ =====
    if needs_llm_correction(rule_results) and llm_client is not None:
        report["llm_used"] = True
        mode = select_llm_mode(rule_results)

        if mode == "full":
            llm_levels = llm_infer_full_structure(raw_headings, rule_results, llm_client)
        else:
            llm_levels = llm_infer_partial(raw_headings, rule_results, llm_client)

        final_results = merge_llm_corrections(rule_results, llm_levels)
    else:
        final_results = rule_results

    # ===== æ„å»ºæ ‘ =====
    sections = build_sections(lines, raw_headings, final_results, code_block_lines)
    root = build_tree_from_sections(sections, doc_id)

    # ===== åå¤„ç† =====
    report["fixes"] = validate_and_fix_tree(root)

    return root, report
```

---

## Part 8: ä¸ç°æœ‰ Pipeline çš„é›†æˆ

æ–°çš„ `build_robust_tree` æ›¿æ¢åŸæœ‰çš„ `build_tree`ï¼Œåç»­æµç¨‹ä¸å˜ï¼š

```python
def build_document(markdown_text: str, doc_id: str, llm_client) -> TreeNode:
    # Step 1: æ„å»ºæ ‘ï¼ˆå‡çº§ç‰ˆï¼‰
    root, report = build_robust_tree(markdown_text, doc_id, llm_client)
    print_build_report(report)  # è¾“å‡ºæ„å»ºæ—¥å¿—

    # Step 2: ç”Ÿæˆ summaryï¼ˆå« Case 2 æ”¹è¿›ï¼‰
    generate_summaries(root, llm_client)

    # Step 3: æ³¨å…¥ preamble
    inject_preamble_leaves(root)

    # Step 4: preamble summary
    generate_preamble_summaries(root, llm_client)

    return root
```

---

## Part 9: ç»ˆç«¯æ„å»ºæ—¥å¿—è¾“å‡º

```
============================================================
ğŸ”¨ Building Tree: test_paper.md
============================================================

>>> Layer 1: Preprocessing
  Lines: 156, Code blocks: 2 (filtered 12 lines)
  Headings detected: 18 (16 with # marker, 2 unmarked)

>>> Layer 2: Rule-based Inference
  High confidence (>= 0.8): 12/18 headings
  Medium confidence (0.6-0.8): 3/18 headings
  Low confidence (< 0.6): 3/18 headings
  âš  Low confidence headings:
    "Background Details" â†’ L1 (conf=0.3, reason: no numbering, all_same #)
    "Data Preprocessing" â†’ L1 (conf=0.3, reason: no numbering, all_same #)
    "Supplementary"      â†’ L1 (conf=0.5, reason: special section but ambiguous)

>>> Layer 3: LLM Correction (partial mode)
  Corrected 3 headings:
    "Background Details"  L1 â†’ L3 (reasoning: sub-topic of 1.1 Background)
    "Data Preprocessing"  L1 â†’ L2 (reasoning: parallel to 2.1 Model Architecture)
    "Supplementary"       L1 â†’ L1 (reasoning: confirmed as top-level appendix)

>>> Post-processing
  Fixes applied: 0
  Warnings: 0

>>> Result
  Nodes: 18, Leaves: 12
  Max depth: 3
  LLM calls: 1
============================================================
```

---

## Part 10: æµ‹è¯•ç”¨ä¾‹çŸ©é˜µ

åˆ›å»ºä»¥ä¸‹æµ‹è¯•æ–‡ä»¶ï¼ŒéªŒè¯å„ç§è¾¹ç•Œæƒ…å†µï¼š

| æ–‡ä»¶å | è¦†ç›–çš„ Case | å…³é”®éªŒè¯ç‚¹ |
|--------|------------|-----------|
| `test_standard.md` | Case 1 | åŸºçº¿ï¼Œ# å’Œç¼–å·ä¸€è‡´ |
| `test_flat_numbered.md` | Case 2, 4 | å…¨ # + ç¼–å·ï¼Œç¼–å·è·³è·ƒ |
| `test_flat_no_number.md` | Case 3 | å…¨ # + æ— ç¼–å·ï¼ˆéœ€è¦ LLMï¼‰ |
| `test_mixed_numbering.md` | Case 6, 7 | ç½—é©¬æ•°å­— + å­—æ¯ + ç‰¹æ®Šç« èŠ‚ |
| `test_noisy.md` | Case 8, 9 | ä»£ç å—ä¼ªæ ‡é¢˜ + æ ¼å¼å™ªéŸ³ |
| `test_level_jump.md` | Case 5, 11 | å±‚çº§è·³è·ƒ + å±‚çº§åè½¬ |
| `test_deep.md` | Case 13 | è¶…è¿‡ 3 å±‚çš„æ·±å±‚åµŒå¥— |
| `test_chinese.md` | Case 10 | ä¸­æ–‡ç¼–å· + ä¸­æ–‡ç« èŠ‚å |

æ¯ä¸ªæµ‹è¯•æ–‡ä»¶ 30-50 è¡Œå³å¯ï¼Œé™„å¸¦é¢„æœŸçš„æ ‘ç»“æ„ï¼ˆnode_count, leaf_count, å„èŠ‚ç‚¹ levelï¼‰ä½œä¸ºæ–­è¨€ã€‚

---

## æ–‡ä»¶ç»“æ„

```
tree_builder/
â”œâ”€â”€ preprocessor.py          # Layer 1: ä»£ç å—è¿‡æ»¤ã€æ ‡é¢˜æ ‡å‡†åŒ–ã€æ— æ ‡è®°æ ‡é¢˜æ£€æµ‹
â”œâ”€â”€ signals.py               # HeadingSignals æ•°æ®ç»“æ„ + ä¿¡å·æå–
â”œâ”€â”€ numbering.py             # ç¼–å·è§£æå™¨ï¼ˆå…¨æ ¼å¼æ”¯æŒï¼‰
â”œâ”€â”€ special_sections.py      # ç‰¹æ®Šç« èŠ‚è¯†åˆ«
â”œâ”€â”€ rule_engine.py           # Layer 2: è§„åˆ™æ¨æ–­å¼•æ“ + DocumentContext
â”œâ”€â”€ llm_corrector.py         # Layer 3: LLM ä¿®æ­£ï¼ˆfull / partial ä¸¤ç§æ¨¡å¼ï¼‰
â”œâ”€â”€ tree.py                  # æ ˆç®—æ³•å»ºæ ‘ + åå¤„ç†éªŒè¯ (å·²æœ‰ï¼Œéœ€ä¿®æ”¹)
â”œâ”€â”€ preamble.py              # Preamble æ³¨å…¥ (æ–°å¢)
â”œâ”€â”€ summary.py               # Summary ç”Ÿæˆï¼ˆå« Case 2 æ”¹è¿›ï¼‰(å·²æœ‰ï¼Œéœ€ä¿®æ”¹)
â”œâ”€â”€ builder.py               # ä¸»å…¥å£: build_robust_tree + build_document
â”œâ”€â”€ visualizer.py            # ç»ˆç«¯æ‰“å° + æ„å»ºæ—¥å¿— (å·²æœ‰ï¼Œéœ€ä¿®æ”¹)
â””â”€â”€ test_data/
    â”œâ”€â”€ test_standard.md
    â”œâ”€â”€ test_flat_numbered.md
    â”œâ”€â”€ test_flat_no_number.md
    â”œâ”€â”€ test_mixed_numbering.md
    â”œâ”€â”€ test_noisy.md
    â”œâ”€â”€ test_level_jump.md
    â”œâ”€â”€ test_deep.md
    â””â”€â”€ test_chinese.md
```

---

## æŠ€æœ¯ä¾èµ–

- Python 3.10+ æ ‡å‡†åº“ï¼ˆre, dataclasses, json, collectionsï¼‰
- LLM è°ƒç”¨: `anthropic` æˆ– `openai` SDKï¼ˆä»… Layer 3 éœ€è¦ï¼Œå¯é€‰ï¼‰
- æ— å…¶ä»–å¤–éƒ¨ä¾èµ–

## Mock æ¨¡å¼

å½“ `llm_client=None` æ—¶ï¼ŒLayer 3 æ•´ä½“è·³è¿‡ï¼Œä»…ä½¿ç”¨ Layer 1 + Layer 2 çš„ç»“æœã€‚å¯¹äºä½ç½®ä¿¡çš„ headingï¼Œåœ¨ report ä¸­æ ‡è®° warning ä½†ä¸ä¿®æ­£ã€‚